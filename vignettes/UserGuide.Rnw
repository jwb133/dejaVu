\documentclass{article}

\usepackage{fullpage}
\usepackage{url}
\usepackage{color}
\usepackage{authblk}
\usepackage{amsmath}
\usepackage{amssymb}

%\VignetteEngine{knitr::knitr}
%\VignetteIndexEntry{User Guide}

\begin{document}

\title{DejaVu User Guide}
\author[**]{Nikolas S. Burkoff}
\author[*]{David Ruau}

\affil[*]{AstraZeneca, B\&I, Advanced Analytics Centre, UK}
\affil[**]{Tessella, 26 The Quadrant, Abingdon Science Park, Abingdon, OX14 3YS, UK}


\renewcommand\Authands{ and }

\maketitle

\section{Introduction}

TODO (not worth writing until I have a better idea of the functionality the package will take)

\section{Running a Single MI Example}

In this section we walk through a single MI example:
\begin{itemize}
\item Simulating Complete Data
\item Simulating Subject Dropout
\item Generating MI data sets
\item Analysing results using Rubin's formula 
\end{itemize}

See later in the guide for details of running a large number of examples and combining the results to estimate summary statistics such as power and Type I error.

In order to use the functionality of the package, it must be loaded:
<<load,message=FALSE>>=
library(dejaVu)
@

\textbf{Note:} due to namespace conflicts between libraries \texttt{MASS} and \texttt{trafficlight}, \texttt{dejaVu} does will not work if \texttt{trafficlight} is loaded. 

For reproducibility we set the random seed:
<<seed>>=
set.seed(1298711)
@

\subsection{Simulating Complete Data}

The \texttt{SimulateComplete} function is used to generate a complete data set of subject outcomes for a recurrent event study with follow up time $T$ and the number of events for each subject, $n_i$ is given by a negative binomial process. 

Under the negative binomial assumption the time interval between adjacent event for a given subject are exponentially distributed with rate $\lambda_i/T$ where $\lambda_i \sim \mathrm{Gamma}(1/k,k\mu)$ and $k$ is the dispersion parameter and $\mu$ the mean number of events ($k$ and $\mu$ can be different for each treatment group)

Specifically
\begin{equation}
\label{nbpdf}
\mathbb{P}(n_i=r) = NB(p,\gamma) = \left.\frac{\Gamma(\gamma+ r)}{\Gamma(r+1)\Gamma(\gamma)}p^r(1-p)^{\gamma}\right.\end{equation} where $\gamma=1/k$ and $p=\frac{k\mu}{1+k\mu}$.


The \texttt{SimulateComplete} function takes the following arguments:
\begin{itemize}
\item \texttt{study.time} $=T$
\item \texttt{number.subjects}: the number of subjects in each arm to simulate - if it is a single number then this will be used for both arms, otherwise a vector \texttt{c(number.control,number.active)}
\item \texttt{event.rates} $=\mu/T$: the event rates for each arm
\item \texttt{dispersions} $=k$, again either a single number if $k$ is the same for both arms otherwise a vector.
\end{itemize}

An example of the data generation procedure:
<<complete>>=
complete <- SimulateComplete(study.time=365, 
                      number.subjects=50, 
                      event.rates=c(0.01,0.005),
                      dispersions=0.25) 
print(complete)
summary(complete)
@

We can also access the data directly:
<<>>=
head(complete$data)

#The event times for subject with Id 1
complete$event.times[[1]]

@

The SimulateComplete function returns a \texttt{SingleSim} object. See \texttt{help(SingleSim)} for further details.

\subsection{Subject Dropout}

Given a \texttt{SingleSim} object, we can use the \texttt{SimulateDropout} function to apply a dropout mechanism to create a data set which includes subject dropout. In this example we use a simple MCAR example. 

The \texttt{ConstantRateDrop} mechanism will give subjects exponentially distributed drop out times with subject specific rate $R$ where $R=$ \texttt{(rate)}$\exp(X_i)$ where $X_i \sim \mathcal{N}(0,$\texttt{var}$)$. If the simulated drop out time is $<T$ then the subject drops out and any events which occur after dropout are not observed.

<<CRdrop>>=
ConstantRateDrop(rate=0.0025,var=1)
@

This dropout mechanism can be used to create a data set with subject dropouts.
<<drop>>=
with.MCAR.dropout <- SimulateDropout(complete,
                    drop.mechanism=ConstantRateDrop(rate=0.0025,
                                                    var=1)) #var by default=0

@

Note the \texttt{censored.time} and \texttt{observed.events} have been updated:
<<dropsummary>>=
summary(with.MCAR.dropout)

head(with.MCAR.dropout$data)
@

See later in the tutorial for other available dropout mechanisms and a description of how to implement your own.   

\subsection{Generating MI data sets}

Given a \texttt{SingleSim} object we can fit a negative binomial model using the \texttt{Simfit} function. \textbf{Note a warning will be displayed if the model fit does not converge this is possible, especially if there are a small number of subjects}. 
<<fit>>=
my.fit <- Simfit(with.MCAR.dropout,
                 equal.dispersion=TRUE)
@

The function \texttt{glm.nb} from the package \texttt{MASS} is used with the following models:
\begin{itemize}
\item \texttt{observed.events} $\sim$ \texttt{arm + offset(log(censored.time))} if \texttt{equal.dispersion} is \texttt{TRUE}
\item \texttt{observed.events} $\sim$ \texttt{offset(log(censored.time))} if \texttt{equal.dispersion} is \texttt{FALSE} and separate models are fitted for each arm 
\end{itemize}

(Note: If covariates are required in the model, it will not be \textit{too} much work to allow users to specify the model formula). Additional arguments to \texttt{Simfit} are passed to the model fitting function. For example \texttt{Simfit(with.MCAR.dropout,maxit=50)} will increase the number of iterations when fitting the model (see \texttt{help(glm.control)} for further details). 

This creates a \texttt{SingleSimFit} object:
<<>>=
class(my.fit)

summary(my.fit)

#Can access the individual elements of the summary object
x <- summary(my.fit)
x$pval
@

We can output $\gamma$ and $p$ for the model fit (see Equation \eqref{nbpdf}) -- they are vectors containing the value for first the control arm and then the active arm:
<<ip>>=
my.fit$impute.parameters$gamma
my.fit$impute.parameters$p
@

Given a \texttt{SingleSimFit} object we can generate a set of imputed data sets:
<<imp>>=
imputed.data.sets <- Impute(fit = my.fit,
                            impute.mechanism = weighted_j2r(trt.weight=0),
                            N=10)

#output the number of subject dropouts in each arm 
imputed.data.sets$dropout
@

The \texttt{Impute} function requires three arguments, the fit, an impute mechanism and $N$ the number of data sets to impute.
We are using the weighted j2r impute mechanism with treatment weight$=0$ which implies missing counts for subjects in both arms will be imputed according to the mean of the control arm conditioned on the number of subject's observed events. See later in the tutorial for further details, other available impute mechanisms and a description of how to implement your own.   

\subsection{Fitting MI datasets}

It is possible to access the individual imputed data sets (note these data sets assume the imputed data is `truth' and hence as far as this data set is concerned the number of subject dropouts is 0):
<<>>=
sixth.data.set <- GetImputedDataSet(imputed.data.sets,index=6)

summary(sixth.data.set)

head(sixth.data.set$data)
@

Note the \texttt{actual.censored.time} column is the time the subject was actually censored at whereas the \texttt{censored.time} column is the time the subject was censored after applying the imputation. Similarly the \texttt{actual.events} is the actual number of events which occurred and \texttt{observed.events} is the number of events which were `observed' after imputation.

We can fit a model to imputed data sets. By default the \texttt{Simfit} function fits a negative binomial model, however, by using the family argument a Poission or quasi-Poisson model can be used instead (the same model formula is used as in the negative binomial case however \texttt{glm} is used rather than \texttt{glm.nb})
<<>>=
sixth.fit <- Simfit(sixth.data.set,
                    family="poisson") 
                  
summary(sixth.fit)
@

It is possible to fit the entire set of imputed data sets in one go, again using the \texttt{Simfit} function:
<<>>=
fitted <- Simfit(imputed.data.sets,
                 family="negbin") #negbin is the default
@

We can create a data frame collating the fitted values:
<<>>=
head(as.data.frame(fitted))
@

We can also summarise the results -- the values shown here are calculated using Rubin's formula \cite{Rubin1987} (Note I am using the formula from the Handling Missing Recurrent Events Data in Clinical Trials document given to me by Mattis).
<<>>=
summary(fitted)
@

\section{Running Complete Scenarios}

In order to calculate the summary statistics such as power it is necessary to repeat the procedure multiple times. In this example we show how to easily replicate and combine the results

First we create a function which outputs a list of the summaries of the fits we are interested in:
<<scenario>>=
example.scenario <- function(){ 

  #simulate a complete data set
  sim <- SimulateComplete(study.time=365,number.subjects=125,
                       event.rates=c(0.01,0.005),dispersions=0.25)

  #take the simulated data set and apply an MCAR dropout mechanism...                         
  sim.with.MCAR.dropout <- SimulateDropout(sim,
                    drop.mechanism=ConstantRateDrop(rate=0.0025)) 

  #fit a Negative Binomial model 
  with.MCAR.fit <- Simfit(sim.with.MCAR.dropout,equal.dispersion=TRUE)

  #we can impute a set of 10 sets following the j2r mechanism using the fit
  impute.data.sets <- Impute(with.MCAR.fit,impute.mechanism = weighted_j2r(trt.weight=0),N=10)

  #we can then fit models to the entire imputed data set
  fit.imputed.set <- Simfit(impute.data.sets)

  #output the summary values
  return(list(MI=summary(fit.imputed.set), #for MI
              dropout=summary(with.MCAR.fit), #for dropout
              complete=summary(Simfit(sim)))) #for complete data set
}
@

Next we run the simulation a large number of times:
<<rep>>=
answer <- replicate(500,example.scenario(),simplify = FALSE)
@

and process the results using the \texttt{extract\_results} function (note the \texttt{extract\_results} is a simple wrapper around the \texttt{CreateScenario} function): 
<<extract>>=

#answer contains a list of lists each containing 3 SimFit objects
names(answer[[1]])
answer[[1]]$MI
names(answer[[2]])
length(answer)

#we can create a list with only the MI results
MI.fits <- lapply(answer,"[[","MI")

MI.fits[[1]]
MI.fits[[2]]

#and create the scenario
MI.answer <- CreateScenario(MI.fits,description="the description of the scenario")


#The extract_results function can be used to both extract the list and
#create the scenario in one go
MI.answer <- extract_results(answer,name="MI",
                            description="Using j2r multiple imputation")
dropout.answer <- extract_results(answer,name="dropout",
                            description="Using no imputation")
complete.answer <- extract_results(answer,name="complete",
                            description="Using complete data sets")




class(MI.answer)
@

We can output a summary of the simulations as a data frame and summarize the results:
<<scenario.df>>=
head(as.data.frame(MI.answer))

summary(dropout.answer)

summary(complete.answer)

#and can access individual elements
x <- summary(MI.answer,use.adjusted.pval=TRUE,alpha=0.025)

#power is calculated as the proportion of replicas which have
#pvalue < alpha
x$power
@

I haven't implemented plotting functions -- there is an argument for not including them directly in the package:
\begin{itemize} 
\item Given the outputs from the summaries above it should be quite easy to plot exactly what is wanted and an example could be included in the vignette and the possibilites of what is wanted is very general
\item As soon as specific plots are added, users typically (though not always..) want to be able to customize them exactly how they would like them to look and adding so many options to the plots that it becomes easy to just give an example and direct users to lattice/ggplot2.
\item We can discuss this further of course.
\end{itemize}

\section{Additional Information}

\subsection{Additional Dropout mechanisms}

Alongside the \texttt{ConstantRateDrop} function. Additional dropout mechanisms have been implemented:

The \texttt{LinearRateChangeDrop} function allows a piecewise exponential drop out function where after $j$ events subjects have drop out rate $R_j$ where $R_j = C_j\exp(X_j)$ where $X_j \sim \mathcal{N}(0,\sigma^2)$ and $C_j = C+jD$ for constants $C$ and $D$.
<<linear>>=
drop.mec <- LinearRateChangeDrop(starting.rate=0.0025, #C in text above
                                 rate.change=0.0005, #D in text above
                                 var=1) #sigma^2 in text above by default var=0

drop.mec

with.MAR.dropout <- SimulateDropout(complete,
                    drop.mechanism=drop.mec)


@

Please let me know of any more you would like!

\paragraph{Implementing new drop out mechanisms (advanced!) - the idea is for a developer to add new mechanisms, though advanced R users could follow these instructions}

It is possible to implement your own drop out mechanisms using the \texttt{CreateNewDropoutMechanism} function. A \texttt{DropoutMechanism} object should be created which contains 5 elements. We show a toy example here and see \texttt{help(DropoutMechanism.object)} and \texttt{help(CreateNewDropoutMechanism)} for further details (again it isn't much work to allow covariates/other columns of the data frame to be used in calculating the dropout times).

In our toy\footnote{this is clearly not a sensible dropout method but shows how to implement dropout mechanisms} example we would like the dropout time to be $0.5T$ if subjects have exactly $W$ events and are on the control arm and $T$ (i.e. no dropouts) for all other subjects  where $W$ is a parameter chosen by the user. 

<<newdropout>>=

#we create a function which returns the new dropout mechanism
my.example.dropout <- function(W){ #W is the user parameter
  
  #First we create a function which must take in two arguments,
  #event.times - a list of a single subject's event times
  #data - a row of the data frame containing the subject details
  #and outputs the time of subject dropout 
  GetDropTime <- function(event.times,data){
    if(data$observed.events==W && data$arm==0){
      return(data$censored.time/2)
    }
    return(data$censored.time)
    
  }
  
  #we create a vector of the columns from the data frame that
  #are used in the GetDropTime function
  cols.needed <- c("censored.time","arm","observed.events")
  
  #we call the CreateNewDropoutMechanism function
  #with the following arguments
  CreateNewDropoutMechanism(type="MNAR", #technically this is MNAR dropout
                            text="Toy example dropout", #the text to be output
                            cols.needed=cols.needed, #see above
                            GetDropTime=GetDropTime, #see above
                            parameters=list(the.value.of.W=W) #The parameters to be output
                            )

}

#we can view the dropout mechanism
my.example.dropout(6)

#we can use it
with.my.dropout <- SimulateDropout(complete,
                    drop.mechanism=my.example.dropout(6))

head(with.my.dropout$data)
@

\subsection{Additional Imputing Mechanisms}

By altering the \texttt{trt.weight} argument, the \texttt{weighted\_j2r} imputing mechanism can be used to generate different imputing mechanisms:

If \texttt{trt.weight = 0} then imputation using this mechanism will follow the jump to reference (j2r) model whereby missing counts for subjects in both arms will be imputed according to the mean of the placebo arm conditioned on the subject's observed number of events

If \texttt{trt.weight = 1} then imputation using this mechanism will follow the MAR model whereby missing counts for subjects in each arm will be imputed according to the event rate of subjects in its treatment group conditioned on the subject's observed number of events

Explicitly, when using the \texttt{weighted\_j2r} function, with \texttt{trt.weight}$=\omega$, $p=(p_c,p_a)$ and $\gamma=(\gamma_c,\gamma_a)$ from the model fit for a subject with $O_i$ observed events and `imputation time' $t_i=T-$censor.time, the number of imputed events is negative binomial given by
\begin{itemize}
\item $NB(p=\frac{p_c t_i}{\gamma_c+p_cT},\gamma=\gamma_c+O_i)$ for subjects in the control arm. 
\item $NB(p=(1-\omega)q_c+\omega q_a,\gamma=\gamma_a+O_i)$ for subjects in the active arm where $q_c=\frac{p_c t_i}{\gamma_a + p_a(T-t_i)+p_c t_i}$ and $q_a=\frac{p_a t_i}{\gamma_a+p_aT}$.
\end{itemize}

Finally if \texttt{trt.weight = 1} an additional argument \texttt{delta} can be included. It should be a vector of length 2, \texttt{c(control.delta,treatment.delta)} and in this case the mean number of expected events for the imputed missing data is multipled by the appropriate delta (and so $p$ is adjusted appropriately):
<<moreimp>>=
  weighted_j2r(trt.weight=1,delta=c(1,1.4))
@




\paragraph{Implementing new imputing mechanisms (advanced!) - the idea is for a developer to add new mechanisms, though advanced R users could follow these instructions}

It is possible to implement your own imputing mechanisms using the \texttt{CreateNewImputeMechanism} function. A \texttt{ImputeMechanism} object should be created which contains 4 elements. We show a toy example here and see \texttt{help(ImputeMechanism.object)} and \texttt{help(CreateNewImputeMechanism)} for further details (again it isn't much work to allow covariates/other columns of the data frame to be used when imputing data).

In our toy\footnote{this is clearly not a sensible imputing method but shows how to implement imputing mechanisms} example we would like subjects in the treatment group to have no imputed events and subjects in the control group to have either 0 or 1 events with a given probability and if subjects have an event it is midway between dropping out and the end of the follow up period. 

<<newimp>>=

#we create a function which returns the new dropout mechanism
my.example.impute <- function(prob){ # parameters for probability control arm having event
  
  
  #We need a function which takes in a SingleSimFit object
  #and returns a list with 2 elements:
  #newevent.times which contains vectors of the imputed event times for each subject
  #new.censored.times, a vector of the times the imputed data subjects dropout
  #(the code in this function has been designed for clarity as does NOT follow 
  #R best practice)
  impute <- function(fit){
    
    #how many subjects are there in the data frame?
    number.of.subjects <- nrow(fit$singleSim$data)
    
    #subject follow up time
    study.time <- fit$singleSim$study.time
    
    #After imputing data, all subjects are followed up
    #for study.time 
    new.censored.times <- rep(study.time,number.of.subjects)
    
    #the imputed event times for each subject
    newevent.times <- list()
    
    #for each subject create a vector of imputed event times
    #if no events are imputed then use numeric(0)
    for(id in 1:number.of.subjects){
   
      #what arm is the subject on?
      arm <- fit$singleSim$data[id,]$arm
      
      if(arm==1){
        #if in treatment group then no new events
        newevent.times[[id]] <- numeric(0)
        break
      }
      
      time.left <- study.time - fit$singleSim$data[id,]$censored.time
        
      #if ti = 0 then subject didn't drop out so 
      #no imputed events
      if(time.left==0){
        newevent.times[[id]] <- numeric(0)
        break
      }  
        
      #Note could access the p, gamma from the fit
      p <-  my.fit$impute.parameters$p
      gamma <- my.fit$impute.parameters$gamma
      
      #did subject have an event?
      is.event <- rbinom(n=1,size = 1,prob=prob)
      if(is.event==0){ #if not...
        newevent.times[[id]] <- numeric(0) 
      }
      else{ #if so
        newevent.times[[id]] <- fit$singleSim$data[id,]$censored.time + 0.5*time.left
      }
    }
    
    #return the appropriate list
    return(list(new.censored.times=new.censored.times,
                newevent.times=newevent.times))
    
  }
  
  #we create a vector of the columns from the data frame that
  #are used in the impute function
  cols.needed <- c("censored.time","arm")
  
  CreateNewImputeMechanism(name="my new impute mechanism", #name for outputting
                    cols.needed=cols.needed, #see above
                    impute=impute, #see above
                    parameters=list(prob.control.arm.have.event=prob)) #extra parameter
}

#we can view the impute mechanism
my.example.impute(0.5)

#we can use it
my.imputed <- Impute(my.fit,impute.mechanism = my.example.impute(0.5),N=10)
@

\bibliographystyle{plain}
\bibliography{userguide}

\end{document}